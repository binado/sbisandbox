{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SBI Sandbox","text":""},{"location":"#about_simulation-based_inference","title":"About Simulation-based Inference","text":"<p>The problem of inference is ubiquitous in modern science. In the past decades, many statistical tools have seen their use consolidated across different scientific fields, either through frequentist or Bayesian approaches.</p> <p>In parallel, the design of more powerful and more efficient computing hardware has allowed for the design of high-fidelity simulations of physical systems with increasing complexity, allowing for the generation of synthetic data from them. However, performing inference from these simulators still remains challenging. In these contexts, the likelihood function is not explicitly calculated, and is instead implicitly defined by the data-generating process implemented by the simulator. The problem of performing inference with these systems has thus been named likelihood-free inference or simulation-based inference (hereafter SBI).</p> <p>In the past few years, the development of more sophisticated Machine Learning techniques, in particular deep neural networks, and the production of specialized hardware for training has given new momentum to the field of SBI. For a high-level overview of the impact of these trends on the emergence of new methods, we refer the reader to this review paper by (Cranmer et al, 2019).</p> <p>The number of scientific publications employing the SBI toolbox in their methodology has been rampant in the last couple of years. We highlight https://simulation-based-inference.org/, an automated aggregator of scientific articles related to the subject and spanning many different fields, such as statistics, economics, neuroscience, astrophysics and cosmology, epidemiology and ecology, and so on. Similarly, the Github repository https://github.com/smsharma/awesome-neural-sbi contains a curated list of publications, tutorials and software packages related to SBI.</p>"},{"location":"#references","title":"References","text":"<p>[1]: Cranmer, Kyle, Johann Brehmer, and Gilles Louppe. \"The frontier of simulation-based inference.\" Proceedings of the National Academy of Sciences 117.48 (2020): 30055-30062.</p> <p>[2]: Tejero-Cantero, Alvaro, et al. \"SBI--A toolkit for simulation-based inference.\" arXiv preprint arXiv:2007.09114 (2020).</p> <p>[3]: Kobyzev, Ivan, Simon JD Prince, and Marcus A. Brubaker. \"Normalizing flows: An introduction and review of current methods.\" IEEE transactions on pattern analysis and machine intelligence 43.11 (2020): 3964-3979.</p> <p>[4]: Lueckmann, Jan-Matthis, et al. \"Benchmarking simulation-based inference.\" International conference on artificial intelligence and statistics. PMLR, 2021.</p>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#publications_using_sbi","title":"Publications using SBI","text":"<p>Check out</p> <ul> <li>simulation-based-inference.org, which aggregates hundreds of scientific papers using Simulation-based Inference in different fields.</li> <li>Awesome Neural SBI, a Github repo containing a curated list of methodological and applied papers, software packages, and tutorials.</li> </ul>"},{"location":"tutorials/introduction/","title":"SBI experiments: an introduction","text":"<p>In this notebook, we will introduce three major Simulation-based inference algorithms powered by neural networks. We will use them to perform Bayesian inference on the toy example of estimating the centre of a multivariate normal distribution.</p> <p>All methods used here will be implemented on top of Pytorch, a popular Python package used for performing GPU-accelerated computations with tensors (a.k.a multi-dimensional arrays) as well as for building neural network models.</p> <p>We will use the SBI algorithms' implementations from the <code>sbi</code> package, which uses Pytorch under the hood.</p>"},{"location":"tutorials/introduction/#the_model","title":"The model","text":"<p>We will choose a model where both parameters and data are sampled from a multivariate normal distribution.</p> <p>The parameters \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^n\\) are sampled from</p> \\[ \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mu=\\boldsymbol{0}_n, \\Sigma_1=\\sigma \\boldsymbol{I}_n),\\] <p>where \\(\\sigma &gt; 0\\), \\(\\boldsymbol{I}_n\\) is the n x n identity matrix and \\(\\boldsymbol{0}_n = \\begin{pmatrix} 0 &amp; \\ldots &amp; 0 \\end{pmatrix}^T \\in \\mathbb{R}^n\\). The data \\(\\boldsymbol{x} \\in \\mathbb{R}^n\\) are generated as follows:</p> \\[ \\boldsymbol{x} \\sim \\mathcal{N}(\\mu=\\boldsymbol{\\theta}, \\Sigma_2=\\sigma \\boldsymbol{I}_n) \\] <p>We hereafter fix \\(\\sigma = 0.01\\).</p>"},{"location":"tutorials/introduction/#the_analytical_posterior","title":"The analytical posterior","text":"<p>The posterior distribution can be calculated analytically for this case. Ignoring normalization factors and terms independent of \\(\\boldsymbol{\\theta}\\), we can write</p> \\[\\begin{align*} \\log p(\\boldsymbol{\\theta} | \\boldsymbol{x}) &amp;\\propto -\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\theta})^T \\Sigma^{-1}_2 (\\boldsymbol{x} -\\boldsymbol{\\theta}) - \\frac{1}{2}\\boldsymbol{\\theta}^T \\Sigma^{-1}_1 \\boldsymbol{\\theta}\\\\ &amp;\\propto \\boldsymbol{\\theta}^T \\Sigma^{-1}_2 \\boldsymbol{x} - \\frac{1}{2}\\boldsymbol{\\theta}^T(\\Sigma^{-1}_1 + \\Sigma^{-1}_2)\\boldsymbol {\\theta}, \\end{align*}\\] <p>which, after completing the square, is found to follow a multivariate normal \\(\\mathcal{N}(\\mu, \\Sigma_T)\\) distribution of mean and convariance matrix</p> \\[ \\Sigma_T = (\\Sigma_1^{-1} + \\Sigma_2^{-1})^{-1} = \\frac{\\sigma}{2} \\boldsymbol{I}_n\\] \\[ \\mu = \\Sigma_T \\Sigma^{-1}_2 \\boldsymbol{x} = \\frac{\\boldsymbol{x}}{2},\\] <p>the last equality being valid when \\(\\Sigma_1 = \\Sigma_2\\).</p> <pre><code># Uncomment the line below to install the package in Google colab\n#!pip install git+https://github.com/binado/sbisandbox.git\n</code></pre> <pre><code>import sys\n\nsys.path.append(\"..\")\n\nimport torch\nfrom torch import Tensor\nimport arviz as az\nfrom sbi import analysis as analysis\nfrom sbi import utils as utils\nfrom sbi.inference import simulate_for_sbi\nfrom sbi.utils.user_input_checks import (\n    check_sbi_inputs,\n    process_prior,\n    process_simulator,\n)\n\n%config InlineBackend.figure_format = 'retina'\naz.style.use(\"arviz-whitegrid\")\n</code></pre> <pre><code>/Users/bernardoveronese/miniconda3/envs/sbibench/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \nFound Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\nthe same time. Both libraries are known to be incompatible and this\ncan cause random crashes or deadlocks on Linux when loaded in the\nsame Python program.\nUsing threadpoolctl may cause crashes or deadlocks. For more\ninformation and possible workarounds, please see\n    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n\n  warnings.warn(msg, RuntimeWarning)\n</code></pre>"},{"location":"tutorials/introduction/#implementing_the_model","title":"Implementing the model","text":"<p>The main ingredient for SBI is the simulator, which serves as a forward model from input parameters \\(\\boldsymbol{\\theta}\\) to data samples \\(\\boldsymbol{x}\\). If the input are drawn from a proposal distribution \\(\\boldsymbol{\\theta}\\), the simulator implicitly defines the mapping \\(\\boldsymbol{x} \\sim p(\\boldsymbol{x} | \\boldsymbol{\\theta})\\).</p> <p>We implement the gaussian model in the two functions below. Multi-dimensional arrays, or tensors, are represented in Pytorch with the <code>torch.tensor</code> object, which is very similar to Numpy's <code>ndarray</code>, while also having support for running on GPUs.</p> <pre><code>ndim = 5  # You can change the number of dimensions\nseed = 1998\ntorch.manual_seed(seed)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x11254c210&gt;\n</code></pre> <pre><code>from torch.distributions import Distribution, MultivariateNormal\n\ncov = 0.01\ntheta_loc = torch.zeros(ndim)\ntheta_precision_matrix = torch.eye(ndim) / cov\nx_precision_matrix = torch.eye(ndim) / cov\n\n\ndef get_prior() -&gt; Distribution:\n    # Return the prior distribution\n    return MultivariateNormal(loc=theta_loc, precision_matrix=theta_precision_matrix)\n\n\ndef gaussian_linear_simulator(theta: Tensor) -&gt; Tensor:\n    # theta is a tensor of shape (num_samples, ndim)\n    return MultivariateNormal(loc=theta, precision_matrix=x_precision_matrix).sample()\n\n\ndef get_true_distribution(x: Tensor) -&gt; MultivariateNormal:\n    precision_matrix = theta_precision_matrix + x_precision_matrix\n    loc = torch.linalg.solve(precision_matrix, x_precision_matrix @ x.squeeze())\n    return MultivariateNormal(loc=loc, precision_matrix=precision_matrix)\n\n\ndef log_prob(theta: Tensor, x: Tensor) -&gt; Tensor:\n    dist = get_true_distribution(x)\n    return dist.log_prob(theta)\n\n\ndef get_true_posterior_samples(num_samples: int, x: Tensor) -&gt; Tensor:\n    # Return samples from the analytical posterior\n    dist = get_true_distribution(x)\n    return dist.expand((num_samples,)).sample()\n</code></pre> <p>Let us define some utility functions which are going to be convenient for generating plots based on our samples:</p> <pre><code>def get_labels_for_var(var: str, n: int):\n    return list(map(lambda i: f\"${var}_{{{i}}}$\", range(1, n + 1)))\n\n\ndef tensor_to_dataset(labels, tensor):\n    return dict(zip(labels, [tensor[..., i] for i in range(tensor.shape[-1])]))\n</code></pre> <p>In the cell below, we use the <code>process_prior</code> and <code>process_simulator</code> helper methods from <code>sbi</code> to convert the prior and simulator into functions whith the appropriate return types for the package. Alternatively, you may use the helper method <code>sbisandbox.utils.validate_model</code>.</p> <pre><code>prior = get_prior()\n# Check prior, return PyTorch prior.\nprior, num_parameters, prior_returns_numpy = process_prior(prior)\n\n# Check simulator, returns PyTorch simulator able to simulate batches.\nsimulator = process_simulator(gaussian_linear_simulator, prior, prior_returns_numpy)\n\n# Consistency check after making ready for sbi.\ncheck_sbi_inputs(simulator, prior)\n</code></pre>"},{"location":"tutorials/introduction/#visualizing_the_data","title":"Visualizing the data","text":"<p>Let us use the <code>simulate_for_sbi</code> helper method to generate samples of \\(\\theta\\) and \\(x\\).</p> <pre><code>num_simulations = 1000\ntheta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=num_simulations)\ntheta.shape, x.shape\n</code></pre> <pre><code>Running 1000 simulations.:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n\n\n\n(torch.Size([1000, 5]), torch.Size([1000, 5]))\n</code></pre> <p>Density plots for our parameter samples:</p> <pre><code>theta_labels = get_labels_for_var(\"\\\\theta\", ndim)\ntheta_dataset = tensor_to_dataset(theta_labels, theta)\n\nplot_kwargs = dict(shade=0.8, show=True)  # grid=(5, 2), figsize=(12, 20), show=True)\naz.plot_density(theta_dataset, var_names=theta_labels, **plot_kwargs)\n</code></pre> <p></p> <pre><code>array([[&lt;Axes: title={'center': '$\\\\theta_{1}$'}&gt;,\n        &lt;Axes: title={'center': '$\\\\theta_{2}$'}&gt;,\n        &lt;Axes: title={'center': '$\\\\theta_{3}$'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\theta_{4}$'}&gt;,\n        &lt;Axes: title={'center': '$\\\\theta_{5}$'}&gt;, &lt;Axes: &gt;]],\n      dtype=object)\n</code></pre> <p>Density plots for the generated data:</p> <pre><code>x_labels = get_labels_for_var(\"x\", ndim)\nx_dataset = tensor_to_dataset(x_labels, x)\n\naz.plot_density(x_dataset, var_names=x_labels, **plot_kwargs)\n</code></pre> <p></p> <pre><code>array([[&lt;Axes: title={'center': '$x_{1}$'}&gt;,\n        &lt;Axes: title={'center': '$x_{2}$'}&gt;,\n        &lt;Axes: title={'center': '$x_{3}$'}&gt;],\n       [&lt;Axes: title={'center': '$x_{4}$'}&gt;,\n        &lt;Axes: title={'center': '$x_{5}$'}&gt;, &lt;Axes: &gt;]], dtype=object)\n</code></pre> <p>The output of prior + simulator is the set of samples</p> \\[\\{(\\boldsymbol{\\theta}_i, \\boldsymbol{x}_i)\\},\\] <p>for \\(i=1, \\ldots, N_{sim}\\). This will be the data used for the training and validation of the neural network, which is are both taken care of by <code>sbi</code>. Higher simulation budgets \\(N_{sim}\\) increase the training time of the network, but normally yield better approximations for the true posterior - although the scaling is not expected to be linear.</p>"},{"location":"tutorials/introduction/#simulator_sanity-checks","title":"Simulator sanity-checks","text":"<p>Let us fix a fiducial \\(\\boldsymbol{\\theta}_0\\) and use it as input to our simulator many times to get independent samples of \\(\\boldsymbol{x} \\sim p(\\boldsymbol{x} | \\boldsymbol{\\theta}_0)\\), and see if we recover the expected normal distribution centered around \\(\\boldsymbol{\\theta}_0\\):</p> <pre><code>theta_0 = torch.zeros(1000, ndim)\nx_of_theta_0 = simulator(theta_0)\nx_labels = get_labels_for_var(\"x\", ndim)\n\n\ndef plot_corner(labels, samples, truths):\n    dataset = tensor_to_dataset(labels, samples)\n    reference_values = tensor_to_dataset(labels, truths)\n    with az.rc_context({\"plot.max_subplots\": ndim**2 + 1}):\n        _ = az.plot_pair(\n            dataset,\n            var_names=labels,\n            marginals=True,\n            reference_values=reference_values,\n            reference_values_kwargs={\"ls\": \"dotted\", \"markersize\": 20, \"color\": \"red\"},\n        )\n\n\nplot_corner(x_labels, x_of_theta_0, theta_0)\n</code></pre> <p></p> <p>Fixing nominal values:</p> <pre><code>%precision 3\nfiducial_theta, fiducial_x = simulate_for_sbi(simulator, prior, 1)\n{\n    \"Fiducial theta\": fiducial_theta.squeeze().tolist(),\n    \"Data point\": fiducial_x.squeeze().tolist(),\n}\n</code></pre> <pre><code>{'Fiducial theta': [0.122, -0.130, 0.202, -0.004, -0.035],\n 'Data point': [-0.036, -0.246, 0.333, -0.174, -0.120]}\n</code></pre>"},{"location":"tutorials/introduction/#performing_simulation-based_inference","title":"Performing Simulation-based inference","text":""},{"location":"tutorials/introduction/#neural_posterior_estimation_npe","title":"Neural Posterior Estimation (NPE)","text":"<p>Neural Posterior Estimation (NPE) consists of training a neural density estimator with a simulated dataset to directly approximate the posterior \\(p(\\boldsymbol{\\theta} | \\boldsymbol{x})\\). One advantage of this approach is that one can directly sample from the amortized approximation of the posterior, without having to perform extra MCMC steps.</p> <p>The estimator is trained to minimize the loss function</p> \\[     \\mathcal{L}(\\boldsymbol{\\phi}) = \\mathbb{E}_{\\boldsymbol{\\theta} \\sim p(\\boldsymbol{\\theta})} \\mathbb{E}_{\\boldsymbol{x} \\sim p(\\boldsymbol{x} | \\boldsymbol{\\theta})} \\left[-\\log q_{\\boldsymbol{\\phi}} (\\boldsymbol{\\theta} | \\boldsymbol{x}) \\right], \\] <p>where \\(\\boldsymbol{\\phi}\\) is the parameter vector of the neural network. The loss function attains a minimum at \\(q_{\\boldsymbol{\\phi}} (\\boldsymbol{\\theta} | \\boldsymbol{x}) = p(\\boldsymbol{\\theta} | \\boldsymbol{x})\\). Indeed, by writing it explicity,</p> \\[     \\mathcal{L} = -\\iint d\\boldsymbol{\\theta} d\\boldsymbol{x}  p(\\boldsymbol{\\theta}) p(\\boldsymbol{x} | \\boldsymbol{\\theta}) \\log q_{\\boldsymbol{\\phi}}(\\boldsymbol{\\theta} | \\boldsymbol{x}), \\] <p>one can apply Bayes' theorem and commute the integrals to write $$ \\begin{split}     \\mathcal{L} &amp;= -\\int d\\boldsymbol{x} p(\\boldsymbol{x}) \\int d\\boldsymbol{\\theta} p(\\boldsymbol{\\theta} | \\boldsymbol{x}) \\log q_{\\boldsymbol{\\phi}}(\\boldsymbol{\\theta} | \\boldsymbol{x}) \\     &amp;=D_{KL}\\left[q_{\\boldsymbol{\\phi}}(\\boldsymbol{\\theta} | \\boldsymbol{x}) \\parallel p(\\boldsymbol{\\theta} | \\boldsymbol{x}) \\right] + \\rm{const}, \\end{split} $$ where the first term is recognized to be the conditional relative entropy between \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{\\theta} | \\boldsymbol{x})\\) and the true posterior distribution \\(p(\\boldsymbol{\\theta} | \\boldsymbol{x})\\), which is zero if and only if the two measures are equal almost everywhere, and positive otherwise. The additional constant term does not depend on  \\(q_{\\boldsymbol{\\phi}}\\) and equals</p> \\[     \\mathbb{E}_{\\boldsymbol{\\theta} \\sim p(\\boldsymbol{\\theta})} \\mathbb{E}_{\\boldsymbol{x} \\sim p(\\boldsymbol{x} | \\boldsymbol{\\theta})} \\left[-\\log p(\\boldsymbol{\\theta} | \\boldsymbol{x}) \\right]. \\] <p>A common implementation of the estimator \\(q_{\\boldsymbol{\\phi}}(\\boldsymbol{\\theta} | \\boldsymbol{x})\\) is with a normalizing flow. In a nutshell, a normalizing flow is based on the idea of a set of invertible and differentible transformations between two distributions. If we consider our generative model as a series of transformations from a simple distribution (e.g. a prior) to a complex distribution (e.g. the posterior), the normalizing flow will attempt to learn the inverse transformations. For a more formal discussion on the subject, we refer the reader to this review paper.</p> <pre><code>from sbi.inference import SNPE\n\ninference = SNPE(prior=prior)\ninference.append_simulations(theta, x)\n</code></pre> <pre><code>&lt;sbi.inference.snpe.snpe_c.SNPE_C at 0x157be57e0&gt;\n</code></pre> <p>We train the neural network with the <code>train</code> method. A number of arguments can be specified to tune the training hyperparameters (see the docs), but we will stick to their default values here.</p> <pre><code>%%time\ndensity_estimator = inference.train()\n</code></pre> <pre><code> Neural network successfully converged after 56 epochs.CPU times: user 23.8 s, sys: 874 ms, total: 24.6 s\nWall time: 15.7 s\n</code></pre> <p>The output of the training is the density estimator, which we can use to build the posterior:</p> <pre><code>posterior = inference.build_posterior(density_estimator=density_estimator)\n</code></pre> <p>Now that we have built an estimator for the posterior probability \\(p(\\boldsymbol{\\theta} | \\boldsymbol{x})\\), we draw a fiducial pair \\((\\boldsymbol{\\theta}_f, \\boldsymbol{x}_f)\\) to serve as our observed data. In the last line, we use the <code>.sample</code> method to sample from the approximate posterior density.</p> <pre><code>%%time\nnum_samples = 1000\nsamples_from_npe = posterior.sample((num_samples,), x=fiducial_x)\n</code></pre> <pre><code>Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\nCPU times: user 120 ms, sys: 9.42 ms, total: 129 ms\nWall time: 127 ms\n</code></pre> <pre><code>plot_corner(theta_labels, samples_from_npe, fiducial_theta.squeeze())\n</code></pre> <p></p>"},{"location":"tutorials/introduction/#neural_likelihood_estimation_nle","title":"Neural Likelihood Estimation (NLE)","text":"<p>Neural Likelihood Estimation (NLE) shares the same philosophy of NPE, but tries to learn the likelihood \\(p(\\boldsymbol{x} | \\boldsymbol{\\theta})\\) instead of the posterior. Let us redo the calculations with this method.</p> <pre><code>from sbi.inference import SNLE\n\ninference = SNLE(prior=prior)\ninference.append_simulations(theta, x)\n</code></pre> <pre><code>&lt;sbi.inference.snle.snle_a.SNLE_A at 0x15b19d690&gt;\n</code></pre> <pre><code>%%time\ndensity_estimator = inference.train()\n</code></pre> <pre><code> Neural network successfully converged after 35 epochs.CPU times: user 14.1 s, sys: 278 ms, total: 14.4 s\nWall time: 8.16 s\n</code></pre> <p>The differences between NPE and NLE start to show once we have trained the neural network and want to build the posterior. The density estimator approximates the likelihood \\(p(\\boldsymbol{x} | \\boldsymbol{\\theta})\\), therefore we need an additional computational step to obtain samples from the posterior \\(p(\\boldsymbol{x} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\\). This may be achieved with standard sampling methods, such as MCMC, rejection sampling, importance sampling, etc. In the context of our toy model, the procedure may seem redundant, as we know the analytical form of the likelihood. However, as we consider more complex models, for which the form of the likelihood is intractable, the SBI approach allows us to relax any analytical approximations (e.g. gaussian distribution) and let the neural network itself learn the non-linear mapping between \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{x}\\).</p> <p>We list below some of the sampling options implemented in the <code>sbi</code> package:</p> <ul> <li>MCMC<ul> <li>Custom implementation of slice sampling</li> <li>Hamiltonian Monte Carlo (HMC or NUTS) via pyro</li> <li>Hamiltonian Monte Carlo (HMC or NUTS) via PyMC</li> </ul> </li> <li>Rejection sampling</li> <li>Importance sampling</li> <li>Variational inference</li> </ul> <pre><code>posterior = inference.build_posterior(\n    density_estimator=density_estimator, sample_with=\"mcmc\"\n)\n</code></pre> <pre><code>/Users/bernardoveronese/miniconda3/envs/sbibench/lib/python3.10/site-packages/sbi/inference/posteriors/mcmc_posterior.py:114: UserWarning: The default value for thinning in MCMC sampling has been changed from 10 to 1. This might cause the results differ from the last benchmark.\n  thin = _process_thin_default(thin)\n</code></pre> <p>Generating samples with MCMC:</p> <pre><code>%%time\nsamples_from_nle = posterior.sample((num_samples,), x=fiducial_x)\n</code></pre> <pre><code>  0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1200 [00:00&lt;?, ?it/s]\n\n\nCPU times: user 1min 17s, sys: 884 ms, total: 1min 18s\nWall time: 1min 24s\n</code></pre> <pre><code>plot_corner(theta_labels, samples_from_nle, fiducial_theta.squeeze())\n</code></pre> <p></p>"},{"location":"tutorials/introduction/#neural_ratio_estimation_nre","title":"Neural Ratio Estimation (NRE)","text":"<p>As we have seen, the output of prior + simulator is the array of pairs \\((\\boldsymbol{x}_i, \\boldsymbol{\\theta}_i)\\) is drawn from the joint distribution</p> \\[     (\\boldsymbol{x}_i, \\boldsymbol{\\theta}_i) \\sim p(\\boldsymbol{x}, \\boldsymbol{\\theta}) = p(\\boldsymbol{x} | \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}) \\] <p>We now consider the shuffled pairs \\((\\boldsymbol{x}_i, \\boldsymbol{\\theta}_j)\\), where \\(\\boldsymbol{x}_i\\) is the output of the forward-modeled input \\(\\boldsymbol{\\theta}_i, \\, i \\neq j\\). These pairs are sampled from the product distribution</p> \\[     (\\boldsymbol{x}_i, \\boldsymbol{\\theta}_j) \\sim p(\\boldsymbol{x}) p(\\boldsymbol{\\theta}) \\] <p>The idea of NRE is to train a classifier to learn the ratio</p> \\[     r(\\boldsymbol{x}, \\boldsymbol{\\theta}) \\equiv \\frac{p(\\boldsymbol{x}, \\boldsymbol{\\theta})}{p(\\boldsymbol{x})p(\\boldsymbol{\\theta})} = \\frac{p(\\boldsymbol{x} | \\boldsymbol{\\theta})}{p(\\boldsymbol{x})},  \\] <p>which is equal to the likelihood-to-evidence ratio. The application of Bayes' theorem makes the connection between \\(r(\\boldsymbol{x}, \\boldsymbol{\\theta})\\) and the Bayesian inverse problem:</p> \\[     r(\\boldsymbol{x}, \\boldsymbol{\\theta}) = \\frac{p(\\boldsymbol{x}, \\boldsymbol{\\theta})}{p(\\boldsymbol{x})} = \\frac{p(\\boldsymbol{\\theta} | \\boldsymbol{x})}{p(\\boldsymbol{\\theta})}. \\] <p>In other words, \\(r(\\boldsymbol{x}, \\boldsymbol{\\theta})\\) equals the posterior-to-prior ratio. Therefore, one can get samples from the posterior distribution of \\(\\boldsymbol{\\theta}\\) from the approximate knowledge of \\(r(\\boldsymbol{x}, \\boldsymbol{\\theta})\\) and prior samples from \\(\\boldsymbol{\\theta}\\).</p> <p>More specifically, the binary classifier \\(d_{\\boldsymbol{\\phi}} (\\boldsymbol{x}, \\boldsymbol{\\theta})\\) with learnable parameters \\(\\boldsymbol{\\phi}\\) is trained to distinguish the \\((\\boldsymbol{x}_i, \\boldsymbol{\\theta}_i)\\) pairs sampled from the joint distribution from their shuffled counterparts. We label pairs with a variable \\(y\\), such that \\(y=1\\) refers to joint pairs, and \\(y=0\\) to shuffled pairs. The classifier is trained to approximate</p> \\[\\begin{equation*} \\begin{split}     d_{\\boldsymbol{\\phi}} (\\boldsymbol{x}, \\boldsymbol{\\theta}) &amp;\\approx p(y=1 | \\boldsymbol{x}, \\boldsymbol{\\theta})\\\\     &amp;= \\frac{p(\\boldsymbol{x}, \\boldsymbol{\\theta} | y = 1) p(y = 1)}{p(\\boldsymbol{x}, \\boldsymbol{\\theta} | y = 0) p(y = 0) + p(\\boldsymbol{x}, \\boldsymbol{\\theta} | y = 1) p(y = 1)}\\\\     &amp;= \\frac{p(\\boldsymbol{x}, \\boldsymbol{\\theta})}{p(\\boldsymbol{x})\\boldsymbol{\\theta} + p(\\boldsymbol{x}, \\boldsymbol{\\theta})}\\\\     &amp;= \\frac{r(\\boldsymbol{x}, \\boldsymbol{\\theta)}}{1 + r(\\boldsymbol{x}, \\boldsymbol{\\theta)}}, \\end{split} \\end{equation*}\\] <p>where we used \\(p(y=0)=p(y=1)=0.5\\).</p> <p>The classifier learns the parameters \\(\\boldsymbol{\\phi}\\) by minimizing the binary-cross entropy, defined as</p> \\[     L(d_{\\boldsymbol{\\phi}}) = - \\int d\\boldsymbol{\\theta} \\int d\\boldsymbol{x} p(\\boldsymbol{x}, \\boldsymbol{\\theta})\\log d_{\\boldsymbol{\\phi}}(\\boldsymbol{x}, \\boldsymbol{\\theta}) - p(\\boldsymbol{x})\\boldsymbol{\\theta}\\log(1-d_{\\boldsymbol{\\phi}}(\\boldsymbol{x}, \\boldsymbol{\\theta})) \\] <pre><code>from sbi.inference import SNRE\n\ninference = SNRE(prior=prior)\ninference.append_simulations(theta, x)\n</code></pre> <pre><code>&lt;sbi.inference.snre.snre_b.SNRE_B at 0x15bf88250&gt;\n</code></pre> <pre><code>%%time \ndensity_estimator = inference.train()\n</code></pre> <pre><code> Neural network successfully converged after 48 epochs.CPU times: user 9.79 s, sys: 302 ms, total: 10.1 s\nWall time: 6.18 s\n</code></pre> <pre><code>%%time \nposterior = inference.build_posterior(\n    density_estimator=density_estimator, sample_with=\"mcmc\"\n)\nsamples_from_nre = posterior.sample((num_samples,), x=fiducial_x)\n</code></pre> <pre><code>  0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\n  0%|          | 0/1200 [00:00&lt;?, ?it/s]\n\n\nCPU times: user 35.5 s, sys: 670 ms, total: 36.2 s\nWall time: 47.7 s\n</code></pre> <pre><code>plot_corner(theta_labels, samples_from_nre, fiducial_theta.squeeze())\n</code></pre> <p></p>"},{"location":"tutorials/introduction/#analyzing_our_results","title":"Analyzing our results","text":"<p>For comparison, we draw samples from the true posterior:</p> <pre><code>samples_from_true_posterior = get_true_posterior_samples(num_samples, fiducial_x)\nplot_corner(theta_labels, samples_from_true_posterior, fiducial_theta.squeeze())\n</code></pre> <p></p> <p>We validate our results using the C2ST metric, where a classifier is trained to distinguish samples of the true posterior from samples of the estimated posterior and returns a score between 0.5 and 1. If the samples are indistinguishable, the classification performance should be random, and the returned score is 0.5. Higher scores indicate that the classifier is better able to learn to distinguish between the two sample populations.</p> <pre><code>from sbi.utils.metrics import c2st\n\nc2st_scores = dict(\n    zip(\n        (\"NPE\", \"NLE\", \"NRE\"),\n        (\n            c2st(samples, samples_from_true_posterior, seed=seed)\n            for samples in (samples_from_npe, samples_from_nle, samples_from_nre)\n        ),\n    )\n)\nprint(c2st_scores)\n</code></pre> <pre><code>{'NPE': tensor([0.6960]), 'NLE': tensor([0.6160]), 'NRE': tensor([0.6690])}\n</code></pre>"},{"location":"tutorials/introduction/#references","title":"References","text":"<p>[1]: Cranmer, Kyle, Johann Brehmer, and Gilles Louppe. \"The frontier of simulation-based inference.\" Proceedings of the National Academy of Sciences 117.48 (2020): 30055-30062.</p> <p>[2]: Tejero-Cantero, Alvaro, et al. \"SBI--A toolkit for simulation-based inference.\" arXiv preprint arXiv:2007.09114 (2020).</p> <p>[3]: Kobyzev, Ivan, Simon JD Prince, and Marcus A. Brubaker. \"Normalizing flows: An introduction and review of current methods.\" IEEE transactions on pattern analysis and machine intelligence 43.11 (2020): 3964-3979.</p> <p>[4]: Lueckmann, Jan-Matthis, et al. \"Benchmarking simulation-based inference.\" International conference on artificial intelligence and statistics. PMLR, 2021.</p>"}]}